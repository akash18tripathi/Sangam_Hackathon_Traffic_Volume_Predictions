{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv('C:/Users/hp/Desktop/Final/Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_weather_type = {\n",
    "    'Clouds':0,\n",
    "    'Clear':1,\n",
    "    'Rain':2,\n",
    "    'Drizzle':3, \n",
    "    'Mist':4,\n",
    "    'Haze':5, \n",
    "    'Fog':6,\n",
    "    'Thunderstorm':7,\n",
    "    'Snow':8, \n",
    "    'Smoke':9\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting is_holiday column to categorical\n",
    "train_df['is_holiday']=train_df['is_holiday'].apply(lambda x: 1 if x!='None' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deleting the rows with duplicated date_time column\n",
    "train_df = train_df[train_df['date_time'].duplicated()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting date_time column to datetime format\n",
    "train_df['date_time']=pd.to_datetime(train_df['date_time'])\n",
    "train_df = train_df.set_index(train_df['date_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping a row with a single weather_type value Squall in the entire dataset of 34k rows\n",
    "train_df=train_df.drop(train_df['date_time']['2013-05-12 02:00:00'],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['weather_type']=train_df['weather_type'].map(d_weather_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 28588/28588 [00:19<00:00, 1475.78it/s]\n"
     ]
    }
   ],
   "source": [
    "#Converting holidays to that particular whole day(24 hours) \n",
    "for i in tqdm(range(len(train_df))):\n",
    "    if train_df['is_holiday'][i]==1:\n",
    "        p = train_df['date_time'].dt.date[i]\n",
    "        s = p.strftime('%Y')+'-'+p.strftime('%m')+'-'+p.strftime('%d')\n",
    "        train_df.loc[train_df['date_time'][s],'is_holiday']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if day is 5th-Saturday or 6th-Sunday , then it is a weekend\n",
    "d1 = {\n",
    "    5:1,\n",
    "    6:1,\n",
    "    1:0,\n",
    "    2:0,\n",
    "    3:0,\n",
    "    4:0,\n",
    "    0:0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating new features\n",
    "train_df['is_weekend']=train_df.date_time.dt.dayofweek\n",
    "train_df['is_weekend']=train_df['is_weekend'].map(d1)\n",
    "\n",
    "train_df['hour']=train_df['date_time'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting only the required features in another dataframe for training\n",
    "train_dataframe=pd.DataFrame()\n",
    "train_dataframe['is_holiday'] = train_df['is_holiday']\n",
    "train_dataframe['is_weekend'] = train_df['is_weekend']\n",
    "train_dataframe['visibility_in_miles'] = train_df['visibility_in_miles']\n",
    "train_dataframe['temperature'] = train_df['temperature']\n",
    "train_dataframe['clouds_all'] = train_df['clouds_all']\n",
    "train_dataframe['weather_type'] = train_df['weather_type']\n",
    "train_dataframe['hour'] = train_df['hour']\n",
    "train_dataframe['rain_p_h']=train_df['rain_p_h']\n",
    "#train_dataframe['snow_p_h']=train_df['snow_p_h']\n",
    "train_dataframe['traffic_volume'] = train_df['traffic_volume']\n",
    "\n",
    "train_dataframe.to_csv('training_dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=train_dataframe.iloc[:,:-1].values\n",
    "y_train=train_dataframe.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:390: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n",
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:390: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "label_encoder_w = LabelEncoder()\n",
    "x_train[:,5] = label_encoder_w.fit_transform(x_train[:,5])\n",
    "onehotencoder_w = OneHotEncoder(categorical_features=[5])\n",
    "x_train = onehotencoder_w.fit_transform(x_train).toarray()\n",
    "\n",
    "x_train=x_train[:,1:]\n",
    "#converting hours column into categorical variable\n",
    "label_encoder_h = LabelEncoder()\n",
    "x_train[:,-2] = label_encoder_h.fit_transform(x_train[:,-2])\n",
    "onehotencoder_h = OneHotEncoder(categorical_features=[-2])\n",
    "x_train = onehotencoder_h.fit_transform(x_train).toarray()\n",
    "\n",
    "#Avoiding the dummy variable trap\n",
    "x_train=x_train[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 14454/14454 [00:25<00:00, 567.86it/s]\n",
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:390: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n",
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:390: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Applying Data preprocessing same as above with the test set\n",
    "test_df=pd.read_csv('C:/Users/hp/Desktop/Final/Test.csv')\n",
    "test_df['is_holiday']=test_df['is_holiday'].apply(lambda x: 1 if x!='None' else 0)\n",
    "test_df['date_time']=pd.to_datetime(test_df['date_time'])\n",
    "\n",
    "test_df = test_df.set_index(test_df['date_time'])\n",
    "\n",
    "test_df['weather_type']=test_df['weather_type'].map(d_weather_type)\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(test_df))):\n",
    "    if test_df['is_holiday'][i]==1:\n",
    "        p = test_df['date_time'].dt.date[i]\n",
    "        s = p.strftime('%Y')+'-'+p.strftime('%m')+'-'+p.strftime('%d')\n",
    "        test_df.loc[test_df['date_time'][s],'is_holiday']=1\n",
    "\n",
    "test_df['is_weekend']=test_df.date_time.dt.dayofweek\n",
    "test_df['is_weekend']=test_df['is_weekend'].map(d1)\n",
    "test_df['hour']=test_df['date_time'].dt.hour\n",
    "\n",
    "test_dataframe=pd.DataFrame()\n",
    "test_dataframe['is_holiday'] = test_df['is_holiday']\n",
    "test_dataframe['is_weekend'] = test_df['is_weekend']\n",
    "test_dataframe['visibility_in_miles'] = test_df['visibility_in_miles']\n",
    "test_dataframe['temperature'] = test_df['temperature']\n",
    "test_dataframe['clouds_all'] = test_df['clouds_all']\n",
    "test_dataframe['weather_type'] = test_df['weather_type']\n",
    "test_dataframe['hour'] = test_df['hour']\n",
    "test_dataframe['rain_p_h']=test_df['rain_p_h']\n",
    "\n",
    "test_dataframe.to_csv('testing_dataframe.csv')\n",
    "\n",
    "x_test=test_dataframe.iloc[:,:].values\n",
    "\n",
    "label_encoder_test_w = LabelEncoder()\n",
    "x_test[:,5] = label_encoder_test_w.fit_transform(x_test[:,5])\n",
    "onehotencoder_test_w = OneHotEncoder(categorical_features=[5])\n",
    "x_test = onehotencoder_test_w.fit_transform(x_test).toarray()\n",
    "x_test=x_test[:,1:]\n",
    "\n",
    "label_encoder_test_h = LabelEncoder()\n",
    "x_test[:,-2] = label_encoder_test_h.fit_transform(x_test[:,-2])\n",
    "onehotencoder_test_h = OneHotEncoder(categorical_features=[-2])\n",
    "x_test = onehotencoder_test_h.fit_transform(x_test).toarray()\n",
    "x_test=x_test[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_scaling\n",
    "scalar = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_x= scalar.fit_transform(x_train)\n",
    "scaled_x_test = scalar.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using random Forest Regression \n",
    "random_model = RandomForestRegressor(n_estimators=200)\n",
    "random_model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_pred=random_model.predict(x_test)\n",
    "random_pred=list(random_pred)\n",
    "random_pred_int = [float(round(i)) for i in random_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acc_99.95716.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Submitting this file gave accuracy of 99.95716\n",
    "submission=pd.DataFrame()\n",
    "submission['date_time']=test_df['date_time']\n",
    "submission['traffic_volume']=random_pred_int\n",
    "submission.to_csv('random_submission_later.csv',index=False)\n",
    "joblib.dump(random_model,'acc_99.95716.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN_model2 = Sequential()\n",
    "# The Input Layer :\n",
    "ANN_model2.add(Dense(128, kernel_initializer='normal',input_dim = scaled_x.shape[1], activation='relu'))\n",
    "# The Hidden Layers :\n",
    "ANN_model2.add(Dropout(0.2))\n",
    "ANN_model2.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "ANN_model2.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "ANN_model2.add(Dropout(0.2))\n",
    "\n",
    "ANN_model2.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "ANN_model2.add(Dropout(0.2))\n",
    "\n",
    "# The Output Layer :\n",
    "ANN_model2.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "# Compile the network :\n",
    "\n",
    "ANN_model2.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "ANN_model2.fit(scaled_x, y_train, epochs=200, batch_size=32, validation_split = 0.1)\n",
    "\n",
    "ann_pred2 = ANN_model2.predict(scaled_x_test)\n",
    "ann_best =[ int(ann_pred2[i]) for i in range(ann_pred2.shape[0])]\n",
    "\n",
    "submission=pd.DataFrame()\n",
    "submission['date_time']=test_df['date_time']\n",
    "submission['traffic_volume']=ann_best\n",
    "submission.to_csv('ann_relu_submission.csv',index=False)\n",
    "\n",
    "joblib.dump(ANN_model2,'acc_99.95899.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above cell May take 10-15 minutes and will give an accuracy of 99.95899 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:390: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n",
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:390: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Removing the visiblity and weather_type column from the features extracted\n",
    "x_train_prev = train_dataframe.iloc[:,[0,1,3,4,6,7]].values\n",
    "x_test_prev =  test_dataframe.iloc[:,[0,1,3,4,6,7]].values\n",
    "\n",
    "onehotencoder_h_prev = OneHotEncoder(categorical_features=[-2])\n",
    "x_train_prev = onehotencoder_h_prev.fit_transform(x_train_prev).toarray()\n",
    "\n",
    "x_train_prev=x_train_prev[:,1:]\n",
    "\n",
    "onehotencoder_h_prev_test = OneHotEncoder(categorical_features=[-2])\n",
    "x_test_prev = onehotencoder_h_prev_test.fit_transform(x_test_prev).toarray()\n",
    "x_test_prev=x_test_prev[:,1:]\n",
    "\n",
    "scaled_x_train_prev=scalar.fit_transform(x_train_prev)\n",
    "scaled_x_test_prev=scalar.transform(x_test_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy = 99.96043\n",
    "from keras.layers import LeakyReLU\n",
    "ANN_leaky_relu = Sequential()\n",
    "# The Input Layer :\n",
    "ANN_leaky_relu.add(Dense(128, kernel_initializer='normal',input_dim = scaled_x_train_prev.shape[1]))\n",
    "ANN_leaky_relu.add(LeakyReLU(alpha=0.05))\n",
    "# The Hidden Layers :\n",
    "ANN_leaky_relu.add(Dropout(0.2))\n",
    "ANN_leaky_relu.add(Dense(256, kernel_initializer='normal'))\n",
    "ANN_leaky_relu.add(LeakyReLU(alpha=0.05))\n",
    "\n",
    "ANN_leaky_relu.add(Dense(256, kernel_initializer='normal'))\n",
    "ANN_leaky_relu.add(LeakyReLU(alpha=0.05))\n",
    "ANN_leaky_relu.add(Dropout(0.2))\n",
    "\n",
    "ANN_leaky_relu.add(Dense(256, kernel_initializer='normal'))\n",
    "ANN_leaky_relu.add(LeakyReLU(alpha=0.05))\n",
    "ANN_leaky_relu.add(Dropout(0.2))\n",
    "\n",
    "# The Output Layer :\n",
    "ANN_leaky_relu.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "# Compile the network :\n",
    "ANN_leaky_relu.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "ANN_leaky_relu.fit(scaled_x_train_prev, y_train, epochs=40, batch_size=32, validation_split = 0.1)\n",
    "\n",
    "ann_leaky_pred = ANN_leaky_relu.predict(scaled_x_test_prev)\n",
    "ann_leaky_pred_list =[ float(ann_leaky_pred[i]) for i in range(ann_leaky_pred.shape[0])]\n",
    "\n",
    "joblib.dump(ANN_leaky_relu,'acc_99.96043.pkl')\n",
    "\n",
    "submission=pd.DataFrame()\n",
    "submission['date_time']=test_df['date_time']\n",
    "submission['traffic_volume']=ann_leaky_pred_list\n",
    "submission.to_csv('ann_leaky_relu_submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above cell may take 10-15 minutes and will give an accuracy of 99.96043"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking the Average of all 3 models for reducing mean_squared_error and then submitting it\n",
    "final = []\n",
    "for i in tqdm(range(len(x_test))):\n",
    "    final.append((ann_best[i]+random_pred_int[i]+ann_leaky_pred_list[i])/3)\n",
    "    \n",
    "submission=pd.DataFrame()\n",
    "submission['date_time']=test_df['date_time']\n",
    "submission['traffic_volume']=final\n",
    "#accuracy: 99.96292\n",
    "submission.to_csv('final.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will submit the final submitted file which gave an accuracy of 99.96242"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
